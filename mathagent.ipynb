{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ngoax/agent/blob/main/mathagent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvLaDxpCmH5Y"
      },
      "source": [
        "# Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Enable colab to access your drive"
      ],
      "metadata": {
        "id": "Dfb_Y4lAOx7z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxqYgjAjV2BY"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Update packages and install cuda drivers"
      ],
      "metadata": {
        "id": "vfU2yu-Dm9Gq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8QCPC4esbQc9",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!sudo apt-get update && sudo apt-get install -y cuda-drivers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check if CUDA drivers are installed properly and GPU is running"
      ],
      "metadata": {
        "id": "DeaXzbwfoMMn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version"
      ],
      "metadata": {
        "id": "hrziGQ23nZZ2",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "-ssv_W_jnYIW",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "id": "wc4IGgAcn6DL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ulFfRMQQgIz"
      },
      "source": [
        "Sometimes colab throws an error for UTF-8, please then execute this code snippet and try again"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76CXF_YD3Poq"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "def getpreferredencoding():\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1S6Q6ROLovlm"
      },
      "source": [
        "Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "630XO-BBZDWo"
      },
      "outputs": [],
      "source": [
        "!pip install langchain\n",
        "!pip install --upgrade --quiet numexpr\n",
        "!pip install -qU langchain-docling\n",
        "!pip install docling\n",
        "!pip install langgraph\n",
        "!pip install --upgrade --quiet text-generation transformers langchainhub sentencepiece jinja2 bitsandbytes accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dependencies for using Ollama"
      ],
      "metadata": {
        "id": "MlD4-gfvgt2O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U ollama\n",
        "!pip install -U langchain-ollama"
      ],
      "metadata": {
        "id": "BAnN7nMfgspa",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dependencies for API usage"
      ],
      "metadata": {
        "id": "5e_3K9oUPGEI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-anthropic"
      ],
      "metadata": {
        "id": "s_h_0oXcPFy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-google-genai"
      ],
      "metadata": {
        "id": "dASeqwsZPWsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PDF generation (for evaluation)"
      ],
      "metadata": {
        "id": "59oSySxUTQo8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install reportlab"
      ],
      "metadata": {
        "id": "cUel9toqTTP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZEIQpqvHA7O"
      },
      "source": [
        "# Tracing using LangSmith"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "maQJ8e26ZYbz"
      },
      "outputs": [],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "os.environ[\"LANGSMITH_PROJECT\"] = \"evaluation\"\n",
        "if \"LANGSMITH_API_KEY\" not in os.environ:\n",
        "  os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass(\"Enter your LangSmith API key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zPw9gmsFacv"
      },
      "source": [
        "# Model Set up"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Claude"
      ],
      "metadata": {
        "id": "f5M-ZrqXZAPb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "if \"ANTHROPIC_API_KEY\" not in os.environ:\n",
        "  os.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass(\"Enter your Anthropic API key: \")"
      ],
      "metadata": {
        "id": "Rwv51ydJZF7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_anthropic import ChatAnthropic\n",
        "\n",
        "claude35 = ChatAnthropic(\n",
        "    model=\"claude-3-5-sonnet-20240620\",\n",
        "    temperature=0,\n",
        "    max_tokens=4096,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        ")"
      ],
      "metadata": {
        "id": "CJbEZl7naI_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_anthropic import ChatAnthropic\n",
        "\n",
        "claude37 = ChatAnthropic(\n",
        "    model=\"claude-3-7-sonnet-20250219\",\n",
        "    temperature=0,\n",
        "    max_tokens=4096,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        ")"
      ],
      "metadata": {
        "id": "Z4b35veIQt9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAouFXgWFgTj"
      },
      "source": [
        "## Gemini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7VCUp8pf-ZY"
      },
      "outputs": [],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google AI API key: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXcGR2o8gZVG"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "gemini2lite = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash-lite\",\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "gemini2exp = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash-exp\",\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2\n",
        ")"
      ],
      "metadata": {
        "id": "HOGBF-4oQ7yg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "gemini15pro = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-pro\",\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2\n",
        ")"
      ],
      "metadata": {
        "id": "AK1gDrwJQ-dW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvPW3eo9GKKO"
      },
      "source": [
        "## Models using Ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "tCUHUOPaIHOO"
      },
      "outputs": [],
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Turn on Flash Attention ([reference](https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/features/optimizations/attention_optimizations.html)) and set context length"
      ],
      "metadata": {
        "id": "4Z-Yf5txpIJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ['OLLAMA_FLASH_ATTENTION'] = \"1\"\n",
        "print(os.environ.get('OLLAMA_FLASH_ATTENTION'))\n",
        "\n",
        "os.environ['OLLAMA_CONTEXT_LENGTH'] = \"4096\"\n",
        "print(os.environ.get('OLLAMA_CONTEXT_LENGTH'))"
      ],
      "metadata": {
        "id": "7Wzt7cIBrK1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run ollama as subprocess to run in background"
      ],
      "metadata": {
        "id": "n02DXGhJo-hJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "39X_ffTbqPOW"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "process = subprocess.Popen(\"OLLAMA_LLAMA_EXTRA_ARGS='--flash-attn' ollama serve\", shell=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Parameters**\n",
        "\n",
        "\n",
        "*   **num_predict** defines the max amount of tokens to be generated. -1 sets this to infinity.\n",
        "*   **Temperature**: lesser value = more precise, deterministic; higher value = more creative\n",
        "*   **num_context**: context length (default: 2048 in Ollama)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IXYjKMX1O8gT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MareXVw1GRUB"
      },
      "source": [
        "### Llama 3.2 3b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "SPIT1J77jAal"
      },
      "outputs": [],
      "source": [
        "!ollama pull llama3.2:3b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rlZNoIy2qthH"
      },
      "outputs": [],
      "source": [
        "from langchain_ollama import ChatOllama\n",
        "llama32 = ChatOllama(\n",
        "    model = \"llama3.2:3b\",\n",
        "    temperature = 0.5,\n",
        "    num_predict = -1,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Llama 3.3 70b"
      ],
      "metadata": {
        "id": "uyFEjpaqEAGo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama pull llama3.3"
      ],
      "metadata": {
        "collapsed": true,
        "id": "cabLS0LvEPuy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_ollama import ChatOllama\n",
        "llama33 = ChatOllama(\n",
        "    model = \"llama3.3\",\n",
        "    temperature = 0.8,\n",
        "    num_predict = 256,\n",
        ")"
      ],
      "metadata": {
        "id": "xq9-EUq6EQOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AC9HjP_K-01"
      },
      "source": [
        "### Watt-tool-70B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "R-D0mCOgK-W_"
      },
      "outputs": [],
      "source": [
        "!ollama pull fireshihtzu/watt-tool-70B-formatted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uyvugXpGL6gI"
      },
      "outputs": [],
      "source": [
        "from langchain_ollama import ChatOllama\n",
        "wattTool70 = ChatOllama(\n",
        "    model = \"fireshihtzu/watt-tool-70B-formatted\",\n",
        "    temperature = 0.8,\n",
        "    num_predict = 256,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78m94oIQPlW9"
      },
      "source": [
        "### Watt-tool-8B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "iQIWd2K8PtfI"
      },
      "outputs": [],
      "source": [
        "!ollama pull kitsonk/watt-tool-8B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQ3ClNbeP5o7"
      },
      "outputs": [],
      "source": [
        "from langchain_ollama import ChatOllama\n",
        "wattTool8 = ChatOllama(\n",
        "    model = \"kitsonk/watt-tool-8B\",\n",
        "    temperature = 0.8,\n",
        "    num_predict = -1,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hermes-2-Pro-Llama-3-8B"
      ],
      "metadata": {
        "id": "prvt8mf7CawJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama pull interstellarninja/hermes-2-pro-llama-3-8b-tools"
      ],
      "metadata": {
        "collapsed": true,
        "id": "BQTqgSDoC37x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_ollama import ChatOllama\n",
        "hermes2pro = ChatOllama(\n",
        "    model = \"interstellarninja/hermes-2-pro-llama-3-8b-tools\",\n",
        "    temperature = 0.8,\n",
        "    num_predict = -1,\n",
        ")"
      ],
      "metadata": {
        "id": "NdDiT0KJC4jU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hermes-2-Theta-Llama-3-8B"
      ],
      "metadata": {
        "id": "q7IkNhUFCnDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama pull interstellarninja/hermes-2-pro-llama-3-8b-tools"
      ],
      "metadata": {
        "id": "lRWS9gP6C-vg",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_ollama import ChatOllama\n",
        "hermes2theta = ChatOllama(\n",
        "    model = \"interstellarninja/hermes-2-pro-llama-3-8b-tools\",\n",
        "    temperature = 0.7,\n",
        "    num_predict = -1,\n",
        ")"
      ],
      "metadata": {
        "id": "xrWyPa4bC_FH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Qwen 2.5 14b"
      ],
      "metadata": {
        "id": "p0NOgZf7PGRA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama pull qwen2.5:14b"
      ],
      "metadata": {
        "collapsed": true,
        "id": "u5Qr-sxPPMPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_ollama import ChatOllama\n",
        "qwen14b = ChatOllama(\n",
        "    model = \"qwen2.5:14b\",\n",
        "    temperature = 0,\n",
        "    num_predict = -1,\n",
        "    num_ctx = 4096,\n",
        ")"
      ],
      "metadata": {
        "id": "iNLK5cHDPJHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Qwen 2.5 7b"
      ],
      "metadata": {
        "id": "_aT_IQjqjj32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama pull qwen2.5:7b"
      ],
      "metadata": {
        "id": "vEFhV1l4cFLv",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_ollama import ChatOllama\n",
        "qwen7b = ChatOllama(\n",
        "    model = \"qwen2.5:7b\",\n",
        "    temperature = 0,\n",
        "    num_predict = -1,\n",
        "    num_ctx = 4096\n",
        ")"
      ],
      "metadata": {
        "id": "5TWdkZuOcOVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHQ_gA6eGcdy"
      },
      "source": [
        "# Tool Set up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XPB0m-ctnRP"
      },
      "source": [
        "Math tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPc85yZTttdR"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numexpr\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "@tool\n",
        "def calculator(expression: str) -> str:\n",
        "    \"\"\"Calculate expression using Python's numexpr library.\n",
        "\n",
        "    Expression should be a single line mathematical expression\n",
        "    that solves the problem.\n",
        "\n",
        "    Examples:\n",
        "        \"37593 * 67\" for \"37593 times 67\"\n",
        "        \"37593**(1/5)\" for \"37593^(1/5)\"\n",
        "    \"\"\"\n",
        "    local_dict = {\"pi\": math.pi, \"e\": math.e}\n",
        "    return str(\n",
        "        numexpr.evaluate(\n",
        "            expression.strip(),\n",
        "            global_dict={},  # restrict access to globals\n",
        "            local_dict=local_dict,  # add common mathematical functions\n",
        "        )\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use Docling Framework to extract file content from PDFs, Word and images  ([Reference](https://docling-project.github.io/docling/examples/custom_convert/))"
      ],
      "metadata": {
        "id": "0GHLQVK8rUKn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urgexISKOaU1"
      },
      "outputs": [],
      "source": [
        "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
        "from docling.datamodel.pipeline_options import PdfPipelineOptions, AcceleratorOptions, AcceleratorDevice\n",
        "from docling.datamodel.base_models import InputFormat\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "from IPython.display import display, Markdown # for Markdown rendering in Colab\n",
        "\n",
        "\n",
        "@tool\n",
        "def pdfFormula(filepath:str) -> str:\n",
        "  \"\"\" Get the content from file at a given filepath using Docling library.\n",
        "\n",
        "  File format can be pdf, docx or an image. OCR is used to extract the text if necessary.\n",
        "\n",
        "  Args:\n",
        "      filepath (str): path to file\n",
        "\n",
        "  Returns:\n",
        "      str: file content in Markdown format represented as string\n",
        "  \"\"\"\n",
        "\n",
        "  pipeline_options = PdfPipelineOptions()\n",
        "\n",
        "  # refine performance settings as necessary\n",
        "  pipeline_options.ocr_options.use_gpu = True\n",
        "  pipeline_options.accelerator_options = AcceleratorOptions(\n",
        "        num_threads=2,\n",
        "        device = AcceleratorDevice.CUDA, # options: AUTO, CPU or CUDA\n",
        "        cuda_use_flash_attention2 = True\n",
        "    )\n",
        "\n",
        "  images_scale: float = 1\n",
        "\n",
        "  converter = DocumentConverter(format_options={\n",
        "      InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)\n",
        "  })\n",
        "\n",
        "  result = converter.convert(filepath)\n",
        "  doc = result.document.export_to_markdown()\n",
        "\n",
        "  return doc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9H11hL6HGMJ"
      },
      "source": [
        "# Math solving agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNdKQ46qczVB"
      },
      "source": [
        "## Agent with create_react_agent ([credit](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9spwr5ruHsF1"
      },
      "source": [
        "Set your query here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QcHRFyvEHrz_"
      },
      "outputs": [],
      "source": [
        "query = \"what is the solution for /content/question.pdf\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPyPDG5qHvpI"
      },
      "source": [
        "Start agent and answer query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5xFG_OGUxqV"
      },
      "outputs": [],
      "source": [
        "from langgraph.prebuilt import create_react_agent\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "import tensorflow as tf\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"\"\"You are a helpful agent tasked to solve mathematical problems.\n",
        "                  If a file is provided, use the `pdfFormula` tool first to extract the math problems.\n",
        "                  For each math problem: call the calculator tool to compute the result, remember the result and proceed with the next problem\n",
        "                  In the end: return all solutions in the following format: Task 1: <solution>\n",
        "                  DONT repeat the instruction. DONT answer in complete sentences. ONLY return the solution\"\"\"),\n",
        "    (\"placeholder\", \"{messages}\"),\n",
        "    (\"user\", \"Remember, always be polite!\"),\n",
        "])\n",
        "\n",
        "tools = [calculator, pdfFormula]\n",
        "graph = create_react_agent(qwen7b, tools=tools, prompt=prompt)\n",
        "inputs = {\"messages\": [(\"user\", query)]}\n",
        "\n",
        "for s in graph.stream(inputs, stream_mode=\"values\"):\n",
        "  message = s[\"messages\"][-1]\n",
        "  if isinstance(message, tuple):\n",
        "    print(message)\n",
        "  else:\n",
        "    message.pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adapted agent for evaluation"
      ],
      "metadata": {
        "id": "Z4CSqCDmU3HF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = claude35"
      ],
      "metadata": {
        "id": "uO1fiRhgSzuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from langgraph.prebuilt import create_react_agent\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "def mathAgent(question: str):\n",
        "  \"\"\"\n",
        "  This function takes a math problem as input and returns the solution.\n",
        "\n",
        "  Args:\n",
        "    question: The math problem to solve.\n",
        "\n",
        "  Returns:\n",
        "    Solution to the math problem.\n",
        "  \"\"\"\n",
        "  prompt = ChatPromptTemplate.from_messages([\n",
        "      (\"system\",  \"\"\"You are a helpful agent tasked to solve mathematical problems.\n",
        "      If a file is provided: use the `pdfFormula` tool first to extract the math problems.\n",
        "      Then: use the calculator tool to compute the answer to the problem.\n",
        "      If there are multiple problems: return the answer for each problem\n",
        "      You are not allowed to calculate things yourself but have to use the calculator tool.\n",
        "      Return the solution from the calculator tool, DONT change the answer.\n",
        "      ONLY return the solution(s) (should be only a number, no metrics for each task). DONT repeat the instruction. DONT answer in complete sentences.\"\"\"),\n",
        "      (\"placeholder\", \"{messages}\"),\n",
        "  ])\n",
        "\n",
        "  tools = [calculator, pdfFormula]\n",
        "  graph = create_react_agent(model, tools=tools, prompt=prompt)\n",
        "  inputs = {\"messages\": [(\"user\", question)]}\n",
        "\n",
        "  solution = None\n",
        "\n",
        "  for s in graph.stream(inputs, stream_mode=\"values\"):\n",
        "    message = s[\"messages\"][-1]\n",
        "    if isinstance(message, tuple):\n",
        "      pass\n",
        "    else:\n",
        "      solution = message.content\n",
        "\n",
        "  return solution"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "xcq-E-xzPavT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agent without tools"
      ],
      "metadata": {
        "id": "xuhGaJVoVHUW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "default_instructions = \"You are a helpful agent tasked to solve mathematical problems. ONLY return the solution (should be only a number, no metrics). DONT repeat the instruction. DONT answer in complete sentences.\"\n",
        "\n",
        "def my_app(question: str, instructions: str = default_instructions) -> str:\n",
        "    return model.invoke(\n",
        "        [\n",
        "            {\"role\": \"system\", \"content\": instructions},\n",
        "            {\"role\": \"user\", \"content\": question},\n",
        "        ]\n",
        "    ).content"
      ],
      "metadata": {
        "id": "2bqIRgYpLQgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "Q2exFliQMxFF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set Up"
      ],
      "metadata": {
        "id": "ttG1gcBEUSnG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Metric for checking if the solution provided by the MathAgent is correct"
      ],
      "metadata": {
        "id": "pPJaQK3MgnkB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def correctness(outputs: dict, reference_outputs: dict) -> bool:\n",
        "  return float(outputs[\"response\"]) == float(reference_outputs[\"output\"])"
      ],
      "metadata": {
        "id": "dnO1xX54NXCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def extractNumbers(text):\n",
        "  numbers = re.findall(r'\\d+\\.?\\d*', text)\n",
        "  return numbers[0]\n",
        "\n",
        "def correctness2(outputs: dict, reference_outputs: dict) -> bool:\n",
        "  outputs = {key: extractNumbers(value) for key, value in outputs.items()}\n",
        "  return float(outputs[\"response\"]) == float(reference_outputs[\"output\"])"
      ],
      "metadata": {
        "id": "T9pc9xoqjd_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wrapper"
      ],
      "metadata": {
        "id": "zZG2ctQN2F5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def wrapper (inputs: str) -> dict:\n",
        "  return {\"response\": mathAgent(inputs[\"input\"])}"
      ],
      "metadata": {
        "id": "U0vrDhYz2CJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PDF Generation"
      ],
      "metadata": {
        "id": "XHI-Wg8bbZDe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PDF Generator to convert content into a pdf file [Credit](https://wiki.ubuntuusers.de/ReportLab/)"
      ],
      "metadata": {
        "id": "vlU-ESckg4Kw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from reportlab.pdfgen import canvas\n",
        "from reportlab.lib.units import cm\n",
        "from reportlab.lib.pagesizes import A4, landscape\n",
        "from reportlab.lib.styles import getSampleStyleSheet\n",
        "from reportlab.platypus import Frame, Paragraph\n",
        "\n",
        "def generatePDF(content, num):\n",
        "  \"\"\"Generates a PDF with the given text.\n",
        "\n",
        "  Args:\n",
        "    text: The text to include in the PDF.\n",
        "\n",
        "  Saves a pdf file in the current directory.\n",
        "  \"\"\"\n",
        "  name = \"question-\" + str(num) + \".pdf\"\n",
        "\n",
        "  pdf = canvas.Canvas(name ,pagesize=A4)\n",
        "  breite, hoehe = A4\n",
        "  style = getSampleStyleSheet()\n",
        "  story = []\n",
        "  f = Frame(1*cm,1*cm,breite-(2*cm),hoehe-(2*cm))\n",
        "  story.append(Paragraph(content, style['BodyText']))\n",
        "  f.addFromList(story,pdf)\n",
        "  pdf.save()"
      ],
      "metadata": {
        "id": "vpV4zUvcwKHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def convertJsonlToPdfs(file_path):\n",
        "  errors = 0\n",
        "  try:\n",
        "      with open(file_path, 'r') as file:\n",
        "          for line_number, line in enumerate(file, start=1):\n",
        "              try:\n",
        "                  json_obj = json.loads(line)\n",
        "                  input_text = json_obj['inputs']['input']\n",
        "                  generatePDF(input_text, line_number)\n",
        "              except json.JSONDecodeError as e:\n",
        "                  print(f\"Skipping invalid JSON line: {line.strip()} - Error: {e}\")\n",
        "                  errors += 1\n",
        "              except KeyError as e:\n",
        "                  print(f\"Skipping line missing 'inputs' or 'input': {line.strip()} - Error: {e}\")\n",
        "                  errors += 1\n",
        "  except FileNotFoundError:\n",
        "      print(f\"File not found: {file_path}\")\n",
        "  print(\"Errors: \", errors)"
      ],
      "metadata": {
        "id": "yIwKBX9ldWgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "convertJsonlToPdfs('evaluation.jsonl')"
      ],
      "metadata": {
        "id": "GmsP2Y_OTZ6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "R_yyxVPWUafV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation on PDF"
      ],
      "metadata": {
        "id": "elN3v0cCUd02"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langsmith import Client\n",
        "\n",
        "client = Client()\n",
        "\n",
        "experiment_results = client.evaluate(\n",
        "    wrapper, # Your AI system\n",
        "    data=\"\", # The data to predict and grade over\n",
        "    evaluators=[correctness2], # The evaluators to score the results\n",
        "    experiment_prefix=\"claude 35 sonnet with tool calling\", # A prefix for your experiment names to easily identify them\n",
        ")"
      ],
      "metadata": {
        "id": "EQ9h03X8gb41",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation on string"
      ],
      "metadata": {
        "id": "wcf_dg6yUf-7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langsmith import Client\n",
        "\n",
        "client = Client()\n",
        "\n",
        "experiment_results = client.evaluate(\n",
        "    wrapper, # Your AI system\n",
        "    data=\"\", # The data to predict and grade over\n",
        "    evaluators=[correctness2], # The evaluators to score the results\n",
        "    experiment_prefix=\"claude35 with tool calling\", # A prefix for your experiment names to easily identify them\n",
        ")"
      ],
      "metadata": {
        "id": "I-nIit2SUnhb",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "oAouFXgWFgTj",
        "MareXVw1GRUB",
        "uyFEjpaqEAGo",
        "0AC9HjP_K-01",
        "78m94oIQPlW9",
        "prvt8mf7CawJ",
        "q7IkNhUFCnDn",
        "p0NOgZf7PGRA",
        "k9H11hL6HGMJ"
      ],
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}